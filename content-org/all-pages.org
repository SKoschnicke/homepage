#+hugo_base_dir: ../

* Pages
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:END:

** Home                                                                          :ATTACH:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:ID:       9ef0075f-3e73-49a7-ab6d-ade5b436f7fa
:END:

#+DOWNLOADED: screenshot @ 2026-01-20 19:52:46
#+ATTR_HTML: :class homepage-image
[[attachment:2026-01-20_19-52-46_screenshot.png]]

Welcome to my little home on the interwebs. I'm Sven Koschnicke, a computer nerd with over 25 years of software-engineering experience across the full technology stack - from hardware design and kernel modules to DevOps, backend services, and frontend applications.

Here you'll find my thoughts on technology, software development, and the occasional deep dive into interesting tools and techniques. Feel free to explore my [[/posts/][posts]] or learn more [[/about/][about me]].

** About
:PROPERTIES:
:EXPORT_FILE_NAME: about
:ID:       1cf09891-23bf-4f31-a572-750cc9453778
:END:

Hey, I'm Sven Koschnicke, a computer nerd with over 25 years of hands-on
experience across the entire technology stack. From designing microchips and
programming FPGAs to developing high-performance kernel modules, I bring deep
technical expertise at every level.

My unique strength lies in true full-stack mastery - seamlessly bridging
hardware, systems, and software. Whether it's DevOps infrastructure,
sophisticated backend services, or polished frontend applications, I've
successfully delivered solutions across the spectrum.

I specialize in tackling complex technical challenges that require innovative
thinking and optimization. With my broad knowledge I can successfully steer a
team to create a robust and scalable software solution. I focus on creating
high-performance, specialized systems that push boundaries and deliver
exceptional value.

If you're looking for someone who can not only understand your technical
challenges from every angle but also architect and implement solutions that
exceed expectations in both performance and quality, let's connect.

You can download my full [[/cv-en.pdf][resume in English]] or [[/cv-de.pdf][in German]].

*** Contact

You can reach me by email: [[mailto:s.koschnicke@gfxpro.com][s.koschnicke@gfxpro.com]].

My postal address:

#+begin_quote
GFXpro, z. Hd. Sven Koschnicke\\
Tannenstraße 16\\
24229 Schwedeneck\\
Germany
#+end_quote

Bank account:

#+begin_quote
bunq\\
IBAN: NL02 BUNQ 2057 0845 58\\
BIC: BUNQNL2A
#+end_quote

*** Legal / Imprint (in german)

Anbieter und Verantwortlicher im Sinne des § 5 TMG und § 55 Absatz 2 RStV ist:

#+begin_quote
Sven Koschnicke (Einzelunternehmer)\\
Tannenstraße 16\\
24229 Schwedeneck\\
Germany\\
E-Mail: [[mailto:s.koschnicke@gfxpro.com][s.koschnicke@gfxpro.com]]\\
USt-IdNr.: DE235896138
#+end_quote

Hinweis für Verbraucher:

Die Europäische Kommission stellt eine Plattform zur Online-Streitbeilegung (OS) bereit: [[https://ec.europa.eu/consumers/odr]]

Ich bin nicht bereit oder verpflichtet, an Streitbeilegungsverfahren vor einer Verbraucherschlichtungsstelle teilzunehmen.

** Imprint
:PROPERTIES:
:EXPORT_FILE_NAME: imprint
:END:

This website is operated by me, Sven Koschnicke, and I am responsible in terms of data protection rights (see my full address on the [[/about/][about page]]).

When using this website, no personal information about you is stored by me.

When your browser-software requests a part of this website, it sends some request information to the server providing the website. This request information includes:

- your current IP-Address over which you are connected to the internet
- name and version of your browser software and operating system
- website, from which you reached the requested website if using a hyperlink (the so called [[https://en.wikipedia.org/wiki/HTTP_referer][referer-URL, it's misspelling originated in the original proposal for the HTTP specification]])
- date and time of the request

The request information is required by the server to process your request of the website (Article 6.1(f) GDPR) and is discarded as soon as this request is fulfilled.

Your privacy is very important to me. For this reason, I do not use any third party services on my website, which would receive your request information. I also do not set any cookies which would allow to track you navigating my website. My website is only available over [[https://en.wikipedia.org/wiki/Transport_Layer_Security][TLS]], the standard for encrypted HTTP (called HTTPS), preventing other from seeing or changing the requested sites (only the hostname, sven.guru, for the request is not encrypted, which is a feature of the [[https://en.wikipedia.org/wiki/Server_Name_Indication][SNI extension]] to TLS and allows the server to serve more than one website over TLS).

When you contact me, I store the personal information which is provided along with your message to me. I keep this data as long as necessary for our conversation (Article 6.1(b) GDPR) and as required by law (which is six years for business correspondence in germany, Article 257(4) german commercial code, HGB). Conversations over e-mail are not encrypted by default and you should use PGP to encrypt e-mails. You can find [[https://keybase.io/skoschnicke][my PGP-key at keybase.io]].

When I work for you, I store your contact data in my invoice system, which is called [[https://www.getharvest.com/privacy-policy][Harvest]] and is operated by Iridesco, LLC d/b/a Harvest, a New York (USA) limited liability company. Harvest is fully GDPR compliant ([[https://www.getharvest.com/privacy-policy][see Harvest's privacy policy]]). Invoices are stored ten years as required by german law (Article 257(4) german commercial code, HGB).

Additionally to normal bank transfer you can pay me using PayPal and Stripe. When using one of these third party services, the service receives your payment data. You can consult the [[https://www.paypal.com/de/webapps/mpp/ua/privacy-prev][PayPal privacy policy]] and the [[https://stripe.com/de/privacy][Stripe privacy policy]] for further details.

In any case, I follow industry standards of technical, physical and administrative security to safeguard your personal information.

If you want to access or change your personal data that I store, you may [[/about/][contact me]]. You have the right to disclosure, correction, deletion or constraint of your personal data, if permitted by law.

If you are unhappy about how I handle your personal data, you may complain to any data privacy authority. For example to the [[https://www.datenschutzzentrum.de/][Independent Centre for Privacy Protection Schleswig-Holstein]].

If you have any questions, feel free to [[/about/][contact me]].
* Posts

** DONE Hello World                                                             :personal:
:PROPERTIES:
:EXPORT_FILE_NAME: hello-world
:EXPORT_DATE: 2025-01-02
:END:

Shipping is a feature, and because I don't have much time these days, I'll start with a minimal viable product and get this homepage published as fast as possible. You'll find my thoughts and insights here soon. Mostly about technology and software development. But I can't say how often or when.

** DONE Magic Wormhole                                                         :software:
:PROPERTIES:
:EXPORT_FILE_NAME: magic-wormhole
:EXPORT_DATE: 2025-05-07
:ID:       d25b0e33-8bd4-44cc-9249-dcfd4eff5b1a
:END:

In a world where software gets more and more complex, and because of that also bloated, there are sometimes little gems which make your day a little bit better. They adhere to the initial goal of software and computers, helping people to get their tasks done. And when they just do their job and then get out of the way, it sometimes makes me smile.

One if these programs I just had the pleasure to use again is Magic Wormhole. A small command line utility which lets you transfer files between two computers.

#+DOWNLOADED: screenshot @ 2025-05-07 16:41:33
[[attachment:2025-05-07_16-41-33_screenshot.png]]
[[https://xkcd.com/949/][XKCD "File transfer"]]

So I wanted to give my laptop SSH access to a server I already had access to from my desktop computer. Of course, login through password is deactivated on the server. So I had to add the public SSH key of my laptop to the =authorized_keys= file on the server.


I could have sent my public key to myself by email or even put it on an USB stick and get it onto the desktop computer like this and then copy it to the server (using the very handy =ssh-copy-id= program), but  I remembered Magic Wormhole, which would let me transfer the key more easily to my desktop computer.

I had it already installed, but it should be easy to install on almost any system (don't ask me about Windows, though). Trying to remember how to use it, I entered ~wormhole --help~ (actually I first tried ~wormhole -h~, but that doesn't work):

#+begin_example
$ wormhole --help
Usage: wormhole [OPTIONS] COMMAND [ARGS]...

  Create a Magic Wormhole and communicate through it.

  Wormholes are created by speaking the same magic CODE in two different
  places at the same time.  Wormholes are secure against anyone who doesn't
  use the same code.

Options:
  --appid APPID                   appid to use
  --relay-url URL                 rendezvous relay to use
  --transit-helper tcp:HOST:PORT  transit relay to use
  --dump-timing FILE.json         (debug) write timing data to file
  --version                       Show the version and exit.
  --help                          Show this message and exit.

Commands:
  help
  receive  Receive a text message, file, or directory (from 'wormhole send')
  send     Send a text message, file, or directory
  ssh      Facilitate sending/receiving SSH public keys
#+end_example

Okay, so executing ~wormhole send .ssh/id_ed25519.pub~ should give me a code-word which I can use on the receiving machine by executing ~wormhole receive~ and get the file securely transferred.

But then the ~ssh~ command caught my attention, as it seemed to be exactly my use-case:

#+begin_example
  $ wormhole ssh --help
  Usage: wormhole ssh [OPTIONS] COMMAND [ARGS]...

    Facilitate sending/receiving SSH public keys

  Options:
    --help  Show this message and exit.

  Commands:
    accept  Send your SSH public-key
    invite  Add a public-key to a ~/.ssh/authorized_keys file
#+end_example

That was perfect. Just executing ~wormhole ssh invite~ on the server:

#+begin_example
  $ wormhole ssh invite
  Now tell the other user to run:

  wormhole ssh accept 1-embezzle-printer
#+end_example

And then running ~wormhole ssh accept 1-embezzle-printer~ on the laptop:

#+begin_example
  $ wormhole ssh accept 1-embezzle-printer
  Sending public key type='ssh-ed25519' keyid='wayreth ssh key'
  Really send public key 'wayreth ssh key' ? [y/N]: y
  Key sent.
#+end_example

And on the server the program exits with:

#+begin_example
  Appended key type='ssh-ed25519' id='wayreth' to '/home/sven/.ssh/authorized_keys'
#+end_example

That's it. Now I could access the server from my laptop. Easy, secure, straight forward.

** TODO Overengineering a static website                                       :web:rust:cloud:unikernel:
:PROPERTIES:
:EXPORT_DATE: 2026-01-22
:END:

Back in the dark ages of the world wide web, when someone wanted to publish a website, they would write some HTML (by hand) and put it into a directory where an Apache webserver could deliver it to the users browser (likely Netscape Navigator). Then, people wanted dynamic content and Apache got an interface to ask a program (e.g. a Perl script) what it should deliver to the user. That was a bit complicated, but it worked. Then came PHP, a script language created only for the purpose of writing dynamic websites and it was directly integrated with Apache through =mod_php=. It was way easier to create dynamic websites now, and everyone did that. Even if they actually didn't need to be dynamic, just to not have to write the navigation on every page again. Today, we have static site generators, that create static HTML from some Markdown sources and it's a little bit like at the beginning again. You could put the static HTML onto GitHub and be done with it. GitHub would deliver the HTML to the user. Easy. I did that with the first versions of this homepage. Write it in emacs (because org-mode is better than Markdown), export it to Markdown for Hugo and let Hugo convert it to HTML.

That works very well. The hugo conversion can even be done in a GitHub workflow on push, so that the HTML artifacts don't have to be part of the repository.

But it's also a little bit boring. I thought about how a generic webserver like Nginx could never be optimized to such extend for serving my static website because it needs to be usable for other use-cases, too. I wanted to try if it would be faster to keep all the pages in memory the way they need to be sent to the client. So I wrote a custom webserver in Rust which gets all the content compiled into the binary on build time. Then I did some benchmarks. The Rust server was over 16 times faster than a standard Nginx server. That was promising, so I added TLS support to the server. When comparing the deployed custom server with GitHub pages, Github pages still wins in overall response speed. That is because the most time of a web request is not spend waiting for the webserver but transferring the data from the server to the client, because the user is rarely sitting right next to the datacenter where the server is located. To make the distance between server and client as short as possible, one uses a content delivery network (CDN). The static content is copied to multiple datacenters all over the world and served from the location nearest to the user. GitHub pages distributes the content through a CDN by default and that makes it faster than any single webserver in one location (except when the user happens to be near this single server). But what a CDN can't do is serve dynamic content. So I added a dynamic component to my homepage in the form of live statistics about requests to the server. It is nothing really useful, but it is something a CDN cannot do.

I could have stopped here. Deploy the Rust server to a virtual private server and be happy. But I didn't want to stop. The server was already a single binary also containing the content it should serve. So why not skip the whole operating system and deploy the server as an unikernel directly to the hypervisor? The most difficult thing was actually finding a cloud provider that supported that and understanding the API for deployment. I choose Hetzner, because they are based in Europe, but it shows that unikernels are a not so often used technology. Using the API successfully needed some experimentation, and the ~ops~ tool to build an unikernel image needs some polishing, but eventually it worked. TLS certificates from Let's Encrypt couldn't be stored on disk anymore, so I had to use a storage bucket for that (Hetzner provides these with a S3 compatible API). Storing the certificates only in memory would have meant that every deploy needs a new certificate, and that way I might have run into rate limits from Let's Encrypt.

But now everything works. This website is served from a custom server running directly on the hypervisor. No system updates I have to worry about (the provider cares about the hypervisor), I only have to update the server itself. And it is fast. I also spend some time reducing the amount of data transferred by optimizing the binary files (images, PDFs). The server supports brotli compression (in addition to the standard gzip compression). So it's still slower than using a CDN if you are not near the datacenter, but everything is properly optimized now and it was a fun learning experience.

Is this kind of deployment actually better? Speed and reduced attack surface are the advantages. I can also add fun little gimmicks like the statistics, because I control the whole webserver, but there are also not so nice things:

- deployment is slow. The binary needs to be build, then the image needs to be build from the binary. Takes about 5 to 10 minutes until the new version is online. That's much for small content changes.
- no easy debugging in production. I can't SSH into the server or look at logs (there are no logs, currently). I'd need to create observability infrastructure like OpenTelemetry and some log consumer where the server could send data. Or I'd need to integrate that into the server, extending it's attack surface again.
- the allocated server resources are way too many for a simple static website. I'm paying about 3.50 euros for the server (2 vCPUs, 4 GB RAM) plus about 6 euros for object storage (that's basically Hetzner's base fee, I'm only using a few kilobytes for the certificates).
- I can't share the server resources to run other services like a Mastodon server or a Matrix server

*** TODO Improvements
[2026-02-07 Sat 13:01]

Interesting project, mediocre presentation. The technical journey deserves better storytelling. Add structure, add specifics, and for the
love of Kernighan, break up those paragraphs.

**** TODO Structure Problems

The whole thing is essentially three giant paragraphs. Wall of text syndrome. My eyes glazed over around the middle of paragraph 2. Break it
up with:
- Subheadings: "The Boring Way", "The Custom Server", "Going Full Unikernel"
- Bullet points for the benchmark results
- Maybe some actual code snippets or architecture diagrams

**** TODO Missing the Good Stuff

You mention "16 times faster than Nginx" but don't show your work. What was the benchmark methodology? What requests per second? What
latency? Numbers without context are just bragging.

The "live statistics" feature sounds interesting but you don't even link to it or show a screenshot. That's your dynamic differentiator —
show it off!

**** TODO The Intro Drags

You spend a whole paragraph on web history before getting to your story. Could be condensed to 2-3 sentences. We know PHP exists.

**** STARTED No Conclusion/Lessons

What did you actually learn? Was it worth it? Would you recommend unikernels? The ending just... stops.
